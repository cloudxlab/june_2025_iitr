## Find Minimum and Maximum of a List

### Explanation:

When you have a list of numbers, sometimes you want to know the **smallest number** (called the *minimum*) and the **largest number** (called the *maximum*) in that list.
For example, in the list `[5, 8, 2, 10, 3]`, the smallest number is `2` and the largest number is `10`.

Finding the minimum and maximum is useful in many real-world cases. For example:

* In exam scores, the minimum score shows the lowest marks obtained by a student, and the maximum shows the highest marks.
* In weather data, the minimum temperature tells the coldest reading of the day, and the maximum shows the hottest reading.

---

### Exercise:

Write a function `find_min_max(numbers)` that takes a list of numbers as input and returns a tuple `(minimum, maximum)`.

---

### Example:

```python
find_min_max([5, 8, 2, 10, 3])  
# Output: (2, 10)

find_min_max([7, 7, 7, 7])  
# Output: (7, 7)
```

---

## Min-Max Normalization

### Explanation:

When we have numbers that are very different in scale (like exam scores out of 100 and salaries in thousands), it becomes hard to compare them directly.
**Min-Max Normalization** is a way to rescale numbers so that they fall in a fixed range, usually **0 to 1**.

The formula is:

$$
\text{normalized value} = \frac{(x - \min)}{(\max - \min)}
$$

* `x` = the original value
* `min` = the smallest value in the dataset
* `max` = the largest value in the dataset

For example, if your dataset is `[10, 20, 30]` and you want to normalize `20`:

$$
\frac{20 - 10}{30 - 10} = \frac{10}{20} = 0.5
$$

So the normalized value is **0.5**.

---

### Exercise:

Write a function `min_max_normalize(value, data)` that:

* Takes two inputs:

  * `value`: the number you want to normalize
  * `data`: a list of numbers (dataset)
* Returns the normalized value of `value` using min-max normalization.

---

### Example:

```python
min_max_normalize(20, [10, 20, 30])  
# Output: 0.5  

min_max_normalize(10, [10, 20, 30])  
# Output: 0.0  
```
---

## *Compute Mean of a List of Numbers*

**Explanation:**
The *mean* (often called the *average*) of a list of numbers is found by adding up all the numbers and then dividing by how many numbers there are.
For example, if we have the numbers `[2, 4, 6, 8]`, the sum is `2 + 4 + 6 + 8 = 20`.
There are `4` numbers in the list.
So, the mean is `20 / 4 = 5`.

This is a useful way to find the central value of a group of numbers.

---

**Exercise:**
Write a function `compute_mean(numbers)` that takes a list of numbers and returns their mean (average).
If the list is empty, return `0`.

---

**Example:**

```python
compute_mean([2, 4, 6, 8])      # Output: 5.0
compute_mean([10, 20, 30])      # Output: 20.0
compute_mean([])                # Output: 0
```
---
## *Compute Standard Deviation (SD) of a List of Numbers*

**Explanation:**
The *standard deviation* (SD) is a way to measure how spread out numbers are in a list.

* If the numbers are very close to each other, the SD will be small.
* If the numbers are spread far apart, the SD will be large.

To compute the SD:

1. Find the *mean* (average) of the numbers.
2. Subtract the mean from each number and square the result.
3. Find the average of these squared differences.
4. Take the square root of that value.

For example, for the list `[2, 4, 4, 4, 5, 5, 7, 9]`:

* Mean = 5
* Squared differences = \[9, 1, 1, 1, 0, 0, 4, 16]
* Average of squared differences = 4
* Standard Deviation = âˆš4 = 2

---

**Exercise:**
Write a function `compute_sd(numbers)` that takes a list of numbers and returns the standard deviation.

---

**Example:**

```python
compute_sd([2, 4, 4, 4, 5, 5, 7, 9])   # Output: 2.0
compute_sd([10, 10, 10, 10])           # Output: 0.0
```
---

## Detect Outliers with Z-Score

### Explanation:

When we analyze a list of numbers, sometimes some values are *much larger* or *much smaller* than the rest. These unusual values are called **outliers**.

One way to detect outliers is by using the **z-score**.
The z-score tells us how many standard deviations a number is away from the mean.

* Formula:

  $$
  z = \frac{(x - \text{mean})}{\text{standard deviation}}
  $$

If the z-score of a number is greater than a threshold (commonly 2 or 3), we call it an outlier.
For example:

* A z-score of 0 means the number is exactly the mean.
* A z-score of 2 means the number is 2 standard deviations away from the mean (possibly an outlier).

---

### Exercise:

Write a function `find_outliers(nums, threshold)` that returns a list of numbers from `nums` that are outliers based on the given z-score threshold.

**Arguments:**

* `nums`: a list of numbers
* `threshold`: a number (like 2 or 3) representing how far from the mean we consider values as outliers

**Return:**

* A list of numbers from `nums` that are outliers.

---

### Example:

```python
find_outliers([10, 12, 12, 13, 12, 11, 90], 2)  
# Output: [90]

find_outliers([5, 6, 7, 8, 9, 10, 100], 3)  
# Output: [100]
```

---
Hereâ€™s a structured Python exercise on **Interquartile Range (IQR):**

---
## Compute IQR

### Explanation:

The **Interquartile Range (IQR)** is a way to measure how spread out the middle values of a dataset are.
It is the difference between the **third quartile (Q3)** and the **first quartile (Q1):**

$$
IQR = Q3 - Q1
$$

* **Q1 (first quartile)** is the value at the 25th percentile of the data (the point where 25% of the data is below it).
* **Q3 (third quartile)** is the value at the 75th percentile of the data (the point where 75% of the data is below it).

The IQR is useful for finding out how spread out the â€œmiddleâ€ 50% of the data is and is often used to detect **outliers**.

---

### Exercise:

Write a function `compute_iqr(data)` that takes a list of numbers and returns the IQR.

**Hints:**

* You can sort the data before finding quartiles.
* Use Pythonâ€™s `numpy.percentile` function or write your own logic.

---

### Example:

```python
compute_iqr([1, 2, 3, 4, 5, 6, 7, 8, 9])  
# Q1 = 3, Q3 = 7 â†’ IQR = 7 - 3 = 4

compute_iqr([10, 20, 30, 40, 50, 60])  
# Q1 = 20, Q3 = 50 â†’ IQR = 50 - 20 = 30
```
---

## Standardization

### Explanation:

In data science and machine learning, different features (columns) can have very different ranges. For example, a person's **age** may range from 0 to 100, while their **salary** may range from 10,000 to 100,000. If we directly use these values, the large numbers (like salaries) may dominate the smaller ones (like age) in calculations.

**Standardization** is a way to bring all values to a similar scale.
We do this by subtracting the **mean** and dividing by the **standard deviation**:

$$
z = \frac{x - \mu}{\sigma}
$$

Where:

* $x$ is the original value,
* $\mu$ is the mean of all values,
* $\sigma$ is the standard deviation of all values,
* $z$ is the standardized value (also called a *z-score*).

This process makes the data have a mean of 0 and a standard deviation of 1.

---

### Exercise:

Write a function `standardize(data)` that takes a list of numbers and returns a new list where each number is standardized.

*Hint:* You will need to calculate the mean and standard deviation first.

---

### Example:

```python
standardize([1, 2, 3, 4, 5])  
# Output: [-1.2649, -0.6325, 0.0, 0.6325, 1.2649]

standardize([10, 10, 10])  
# Output: [0.0, 0.0, 0.0]   (because all values are the same)
```

---

## *Compute Root Mean Square Error (RMSE) in n-dimensions*

**Explanation:**
When we try to measure how close our predicted values are to the actual values, we use something called the *Root Mean Square Error (RMSE)*. It tells us, on average, how far our predictions are from the actual values.

The RMSE is calculated in three steps:

1. Subtract each predicted value from the actual value to find the **error**.
2. Square each error (to make them positive).
3. Find the **average** of these squared errors.
4. Take the **square root** of this average.

In *n-dimensions*, both the actual and predicted values are given as lists (or vectors). For example, if the actual values are `[2, 3, 4]` and the predicted values are `[3, 2, 5]`, the errors are `[2-3, 3-2, 4-5] = [-1, 1, -1]`. Squaring them gives `[1, 1, 1]`. The average is `1`, and the square root of `1` is `1`. So, the RMSE is `1`.

---

**Exercise:**
Write a function `compute_rmse(actual, predicted)` that:

* Takes two lists of equal length: `actual` and `predicted`.
* Returns the Root Mean Square Error between them.

---

**Example Usage:**

```python
compute_rmse([2, 3, 4], [3, 2, 5])  
# Output: 1.0  

compute_rmse([1, 2, 3], [1, 2, 3])  
# Output: 0.0  

compute_rmse([2, 3, 4], [3, 1, 7])

```
---
## *Compute Mean Absolute Error in N-Dimensions*

**Explanation:**
The **Mean Absolute Error (MAE)** is a way of measuring how far our predictions are from the actual values.
It is calculated by taking the **average of the absolute differences** between predicted values and actual values.

For example, if the true values are `[3, 5, 2]` and the predicted values are `[2, 5, 4]`, the absolute errors are:

* |3 - 2| = 1
* |5 - 5| = 0
* |2 - 4| = 2

So the MAE is:

$$
\text{MAE} = \frac{1 + 0 + 2}{3} = 1
$$

In **N-dimensions**, each point can have multiple coordinates. The error is computed for each coordinate, then averaged across all points.

---

**Exercise:**
Write a function `compute_mae(actual, predicted)` that:

* Takes two lists of points (each point is a list of numbers, e.g. `[x, y, z, ...]`).
* Returns the mean absolute error across all dimensions.

---

**Example:**

```python
# Example 1: 1-D
actual = [3, 5, 2]
predicted = [2, 5, 4]
compute_mae(actual, predicted)  
# Output: 1.0

# Example 2: 2-D
actual = [[1, 2], [3, 4], [5, 6]]
predicted = [[2, 2], [2, 5], [5, 7]]
compute_mae(actual, predicted)  
# Output: 0.8888888888888888
```

ðŸ‘‰ In the second example, the function computes absolute differences for each coordinate, sums them, and divides by total number of values (not just points).

---

## *Compute Huber Loss for a Dataset*

**Explanation:**
When we build a machine learning model, we test it on many data points. To see how well the model is doing, we calculate the **loss** (difference between prediction and actual value).

The **Huber Loss** is a special function that:

* Uses **squared error** when the prediction is close to the actual value (small error).
* Uses **absolute error** when the prediction is far away (big error).

This makes it less sensitive to outliers than plain squared error.

The formula for one prediction is:

$$
L(y, f) = 
\begin{cases} 
0.5 \cdot (y - f)^2 & \text{if } |y - f| \leq \delta \\ 
\delta \cdot |y - f| - 0.5 \cdot \delta^2 & \text{if } |y - f| > \delta 
\end{cases}
$$

To find the loss for a dataset:

1. Compute this formula for each pair of actual and predicted values.
2. Add them all up.
3. Divide by the number of data points to get the **average loss**.

---

**Exercise:**
Write a function `compute_huber_loss(y_true, y_pred, delta)` that takes:

* `y_true`: a list of actual values
* `y_pred`: a list of predicted values
* `delta`: the threshold value

and returns the **average Huber loss**.

---

**Hints for Beginners:**

1. Start with an empty list to store the loss for each data point.
2. Loop over both `y_true` and `y_pred` together (hint: use `zip`).
3. For each pair `(y, f)` do the following:

   * Calculate the error: `error = y - f`.
   * If `abs(error) <= delta`, compute `0.5 * error**2`.
   * Otherwise, compute `delta * abs(error) - 0.5 * delta**2`.
   * Append this value to your list.
4. At the end, find the average by dividing the sum of all losses by the length of the list.

---

**Example Usage:**

```python
compute_huber_loss([5, 2, 7], [4.8, 2.5, 10], 1)
# For (5, 4.8): 0.5*(0.2^2) = 0.02
# For (2, 2.5): 0.5*(0.5^2) = 0.125
# For (7, 10): 1*3 - 0.5*1^2 = 2.5
# Average = (0.02 + 0.125 + 2.5) / 3 = 0.8817
# Output: 0.8817 (approximately)

compute_huber_loss([1, 2, 3], [1, 2, 3], 1)
# All predictions are exact â†’ loss = 0
# Output: 0
```

---
## *Check if a Point is Closer to A or B in N-Dimension*

**Explanation:**
In mathematics, a *point* in an N-dimensional space can be represented as a list (or tuple) of numbers. For example:

* A point in 2D is like `(x, y)` â†’ e.g., `(3, 4)`
* A point in 3D is like `(x, y, z)` â†’ e.g., `(2, 1, 5)`
* In N-dimensions, itâ€™s just a list with N numbers.

To find how close two points are, we calculate the **distance** between them. The most common way is using the **Euclidean distance** formula:

$$
\text{distance}(P, A) = \sqrt{(p_1 - a_1)^2 + (p_2 - a_2)^2 + ... + (p_n - a_n)^2}
$$

We can compare the distance from point `P` to `A` and from `P` to `B`.

* If `distance(P, A) < distance(P, B)`, then P is closer to A.
* Otherwise, P is closer to B (or equally distant if they are the same).

---

**Exercise:**
Write a function `closer_point(P, A, B)` that:

* Takes three points (`P`, `A`, and `B`) as lists of numbers (same length).
* Returns `"A"` if P is closer to A, `"B"` if P is closer to B, or `"Equal"` if the distances are the same.

---

### **Example:**

```python
closer_point([1, 2], [0, 0], [5, 5])  
# Output: "A"  (because P is closer to A)

closer_point([3, 3, 3], [0, 0, 0], [6, 6, 6])  
# Output: "Equal"  (distances are the same)

closer_point([10, 10], [2, 2], [20, 20])  
# Output: "B"  (P is closer to B)
```

Would you like me to also include a **step-by-step solution code** (with comments for learners), or keep it just as an exercise statement?

----
## *Find Nearest Neighbour in 1D*

### **Explanation:**
Imagine you have a list of numbers on a line (like houses along a street). If you are standing at a certain position (a target number), you may want to know which house (number) is closest to you. This closest number is called the *nearest neighbour*.

For example, if the numbers are `[2, 5, 8, 12]` and the target is `6`, the nearest neighbour is `5` because the distance from `6` to `5` is `1`, while the distance from `6` to `8` is `2`.

We measure distance as the absolute difference:
`distance = |target - number|`

---

### **Exercise:**
Write a function `find_nearest_neighbour(numbers, target)` that returns the nearest neighbour of the target from the list.


### **Example:**

```python
find_nearest_neighbour([2, 5, 8, 12], 6)   # Output: 5
find_nearest_neighbour([1, 4, 10, 20], 15) # Output: 10
```

---
## *Find Nearest Neighbour in 2D*

**Explanation:**
Imagine you are standing on a map at a certain location, and there are several other points (like stores or landmarks) around you. To figure out which one is closest, you measure the *distance* from your location to each of the other points. The point with the smallest distance is called the **nearest neighbour**.

In 2D, each point has two coordinates:

* `x` (horizontal position)
* `y` (vertical position)

The **distance** between two points `(x1, y1)` and `(x2, y2)` is calculated using the **Euclidean distance formula**:

$$
\text{distance} = \sqrt{(x2 - x1)^2 + (y2 - y1)^2}
$$

For example, the distance between `(0, 0)` and `(3, 4)` is:

$$
\sqrt{(3-0)^2 + (4-0)^2} = \sqrt{9 + 16} = 5
$$

---

**Exercise:**
Write a function `find_nearest_neighbour(points, target)` that returns the point from the list `points` which is closest to the `target`.

* `points` is a list of `(x, y)` tuples.
* `target` is a tuple `(x, y)` representing the location we want to compare.
* The function should return the nearest neighbour point as a tuple.

---

**Example Usage:**

```python
find_nearest_neighbour([(1, 2), (3, 4), (6, 1)], (2, 3))
# Output: (1, 2)

find_nearest_neighbour([(0, 0), (5, 5), (2, 1)], (3, 3))
# Output: (2, 1)
```

---

## *Find Nearest Neighbour in N-Dimensions*

**Explanation:**
Imagine you are standing in a city and want to know which shop is closest to you. To find this, you calculate the *distance* between your location and each shopâ€™s location, and then pick the one with the smallest distance.

In computer science, we represent these locations as *points* with coordinates.

* In **2D**, a point might look like `[x, y]` (like a map).
* In **3D**, it could be `[x, y, z]` (like in space).
* In general, in **N dimensions**, a point is `[a1, a2, ..., an]`.

The **Euclidean distance** formula helps us measure how far apart two points are:

$$
\text{distance}(A, B) = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_n)^2}
$$

The nearest neighbour of a point is simply the one with the *smallest distance*.

---

**Exercise:**
Write a function `find_nearest_neighbour(point, points)` that takes:

* `point`: a list of numbers representing coordinates of a point in N-dimensions
* `points`: a list of points (each a list of numbers)

and returns the point from `points` that is nearest to `point`.

---

### Hints (Step-by-Step)

1. **How to calculate distance between two points?**

   * Subtract each coordinate of one point from the corresponding coordinate of the other.
   * Square these differences.
   * Add them all together.
   * Take the square root of the result.

   Example: distance between `[1,2]` and `[3,4]` is
   $\sqrt{(1-3)^2 + (2-4)^2} = \sqrt{4+4} = \sqrt{8}$.

---

2. **How to apply this to a list of points?**

   * For each point in the list, compute its distance to the given point.
   * Keep track of the *smallest distance* found so far.
   * Also remember the corresponding point.

---

3. **How to find the nearest neighbour?**

   * After checking all the points, the one with the smallest distance is the nearest neighbour.

---

**Example Usage:**

```python
find_nearest_neighbour([1, 2], [[3, 4], [2, 1], [0, 0]])
# Output: [2, 1]

find_nearest_neighbour([0, 0, 0], [[1, 1, 1], [2, 2, 2], [-1, -1, -1]])
# Output: [1, 1, 1]
```

---

## Find Nearest Neighbour in N-Dimensions* with custom distance

### Explanation:

Imagine you have a bunch of points in space (like dots on a graph). Each point can have many coordinates, not just two (x, y). For example, in 3-dimensions, a point might be (2, 3, 5). In general, we call this *N-dimensional space*.

The **nearest neighbour problem** means:
Given one target point, find the point (from a list of points) that is closest to it.

The closeness between two points depends on the **distance formula** we use. For example:

* **Euclidean distance**: the straight-line distance between points.
* **Manhattan distance**: the sum of absolute differences in each coordinate.

To make our function flexible, weâ€™ll pass the *distance function* itself as an argument. That way, the learner can plug in any formula they want.

---

### Problem Statement:

Write a function `find_nearest_neighbour(target, points, distance_func)` that:

* Takes:

  * `target`: a list/tuple of numbers representing the target point (e.g., `[1, 2, 3]`).
  * `points`: a list of points (each point is also a list/tuple of numbers).
  * `distance_func`: a function that takes two points and returns the distance between them.
* Returns:

  * The point from `points` that is closest to the `target`.

---

### Example Usage:

```python
# Example distance functions
def euclidean_distance(p1, p2):
    return sum((a - b) ** 2 for a, b in zip(p1, p2)) ** 0.5

def manhattan_distance(p1, p2):
    return sum(abs(a - b) for a, b in zip(p1, p2))


# Using the nearest neighbour function
find_nearest_neighbour([1, 2], [[3, 4], [2, 2], [0, 0]], euclidean_distance)
# Output: [2, 2]  (because itâ€™s the closest in Euclidean sense)

find_nearest_neighbour([1, 2, 3], [[5, 5, 5], [0, 0, 0], [2, 2, 2]], manhattan_distance)
# Output: [2, 2, 2]  (because itâ€™s the closest in Manhattan sense)
```
---

## Multiply a Polynomial with a Number

### Explanation:

A **polynomial** is a mathematical expression consisting of terms like $2x^3 + 3x + 10$. In programming, we can represent a polynomial as a **list of coefficients**, starting from the highest power of $x$ to the constant term.

For example:

$$
2x^3 + 0x^2 + 3x + 10
$$

is represented as `[2, 0, 3, 10]`.

When we multiply a polynomial by a number, each coefficient in the list gets multiplied by that number.

For example:

$$
(2x^3 + 3x + 10) \times 5 = 10x^3 + 15x + 50
$$

In list form: `[2, 0, 3, 10] * 5 = [10, 0, 15, 50]`.

---

### Exercise:

Write a function `multiply_polynomial(poly, num)` that takes:

* `poly`: a list of coefficients of the polynomial
* `num`: a number to multiply the polynomial with

The function should return a new list of coefficients after multiplication.

---

### Example:

```python
multiply_polynomial([2, 0, 3, 10], 5)   # Output: [10, 0, 15, 50]  
multiply_polynomial([1, -2, 4], 3)     # Output: [3, -6, 12]  
```
---
## Add Two Polynomials

### Explanation:

A **polynomial** is a mathematical expression made up of variables and coefficients. For example:

$$
2x^3 + 3x + 10
$$

This polynomial can be written as a list of coefficients, where the index represents the power of $x$.

* $2x^3 + 3x + 10$ becomes `[2, 0, 3, 10]`.

  * `2` â†’ coefficient of $x^3$
  * `0` â†’ coefficient of $x^2$
  * `3` â†’ coefficient of $x^1$
  * `10` â†’ coefficient of $x^0$

To **add two polynomials**, we just add their coefficients for matching powers of $x$.
Example:

$$
(2x^2 + 3x + 4) + (x^2 + 5x + 6) = 3x^2 + 8x + 10
$$

In list form:
`[2, 3, 4] + [1, 5, 6] = [3, 8, 10]`.

---

### Exercise:

Write a function `add_polynomials(p1, p2)` that takes two lists `p1` and `p2`, representing two polynomials, and returns a new list with their sum.

---

### Example:

```python
add_polynomials([2, 0, 3, 10], [1, 4, 0, 6])  
# Output: [3, 4, 3, 16]  
# Explanation: (2x^3 + 3x + 10) + (1x^3 + 4x^2 + 6) = 3x^3 + 4x^2 + 3x + 16

add_polynomials([5, 2], [3])  
# Output: [5, 2+3] = [5, 5]  
# Explanation: (5x + 2) + (3) = 5x + 5
```
---
## Multiply Two Polynomials

### Explanation:

A **polynomial** is an expression made up of variables and coefficients, combined using addition, subtraction, and multiplication.
For example:

$$
2x^3 + 3x + 10
$$

This polynomial can be represented as a list of its coefficients in descending powers of $x$:

$$
[2, 0, 3, 10]
$$

* $2x^3 \rightarrow 2$
* $0x^2 \rightarrow 0$
* $3x^1 \rightarrow 3$
* $10x^0 \rightarrow 10$

When multiplying two polynomials, each term from the first polynomial multiplies with each term from the second, and then like terms (same power of $x$) are added together.

For example:

$$
(2x + 3) \times (x + 4) = 2x^2 + 8x + 3x + 12 = 2x^2 + 11x + 12
$$

which corresponds to $[2, 11, 12]$.

---

### Exercise:

Write a function `multiply_polynomials(p1, p2)` that takes two lists of coefficients `p1` and `p2`, and returns a new list representing their product.

---

### Example:

```python
multiply_polynomials([2, 3], [1, 4])  
# (2x + 3) * (x + 4) = [2, 11, 12]

multiply_polynomials([2, 0, 3, 10], [1, 2])  
# (2x^3 + 3x + 10) * (x + 2) = 2x^4 + 4x^3 + 3x^2 + 16x + 20
# Output: [2, 4, 3, 16, 20]
```
---

---
## *Solve for One Variable in a Linear Equation*

**Explanation:**
A linear equation with multiple variables looks like this:

$$
a_1x_1 + a_2x_2 + a_3x_3 + \dots + a_nx_n = b
$$

Here, each $a_i$ is a coefficient, $x_i$ is a variable, and $b$ is the right-hand side value.
If you already know the values of all variables except one, you can find the unknown variable by rearranging the equation.

For example, consider the equation:

$$
3x + 4y + 6z = 20
$$

If you know $y = 5$ and $z = 6$, then substitute these values:

$$
3x + 4(5) + 6(6) = 20
$$

$$
3x + 20 + 36 = 20
$$

$$
3x = 20 - 56 = -36
$$

$$
x = -12
$$

We represent this using two lists:

* `equation = [3, 4, 6, 20]` â†’ coefficients and RHS value.
* `vars = [5, 6]` â†’ known variable values (in the same order as coefficients after the first one).

So the formula is:

$$
x = \frac{b - (a_2 \cdot var_1 + a_3 \cdot var_2 + \dots)}{a_1}
$$

---

**Exercise:**
Write a function `solve_for_first_variable(equation, vars)` that returns the value of the first variable.

* `equation`: a list of numbers where the first $n$ elements are coefficients of the variables and the last element is the right-hand side value.
* `vars`: a list of values for the last $n-1$ variables.

---

**Example:**

```python
solve_for_first_variable([3, 4, 6, 20], [5, 6])  
# Output: -12  

solve_for_first_variable([2, 5, 7], [4])  
# Equation: 2x + 5y = 7, y=4  
# (7 - (5*4)) / 2 = (7 - 20) / 2 = -13/2 = -6.5  
# Output: -6.5
```

---
## *Eliminate a Variable from two Equations*

**Explanation:**
In algebra, one way to solve systems of equations is called *elimination*.
The idea is to combine two equations in such a way that one variable â€œdisappears.â€ This leaves us with an equation that has one fewer variable, which is easier to solve.

For example, consider these two equations:

1. `2x + 3y = 8`
2. `4x - y = 2`

We want to eliminate `x`.

**Step 1:** Look at the coefficients of `x`. They are 2 (from the first equation) and 4 (from the second).
**Step 2:** Multiply the first equation by 2, so the coefficient of `x` becomes 4.

* Equation 1 becomes: `4x + 6y = 16`
  **Step 3:** Subtract Equation 2 from the new Equation 1:
* `(4x + 6y) - (4x - y) = 16 - 2`
* `7y = 14`

Now, the new equation has only one variable (`y`).

---

**Exercise:**
Write a function `eliminate_variable(eq1, eq2, var_index)` that eliminates the variable at position `var_index` from the two given equations.

* Each equation is represented as a list of numbers: the coefficients of variables followed by the constant term.
* Example: The equation `2x + 3y = 8` is represented as `[2, 3, 8]`.
* The function should return a new equation (list) with one fewer variable.

**Hints for Learners:**

1. Find the coefficients of the variable you want to eliminate in both equations.
2. Multiply each equation so that these coefficients become equal.
3. Subtract one equation from the other to cancel out the chosen variable.
4. The result is a new equation with one fewer variable.

---

**Example Usage:**

```python
eliminate_variable([2, 3, 8], [4, -1, 2], 0)
# Output: [7, 14]   # Represents 7y = 14

eliminate_variable([1, 2, 3], [3, 1, 7], 1)
# Output: [-5, -11]   # Represents -5x = -11
```
---
Got it âœ…
Hereâ€™s a **well-structured Python exercise** for your topic about eliminating one variable from a system of equations.

---

## Eliminate One Variable in Linear Equations

### Explanation

In algebra, a **system of linear equations** is a set of equations with the same variables. For example:

* $2x + 3y = 5$
* $x - y = 10$

These equations involve two variables, $x$ and $y$.

One useful method in solving such systems is **elimination**â€”we combine equations in a way that cancels out (eliminates) one variable. After elimination, the system reduces from $n$ equations with $n$ variables to $n-1$ equations with $n-1$ variables.

In this exercise, equations are represented as a list of lists, where each inner list contains the coefficients of the variables and the constant term at the end.
For example:

$$
2x + 3y = 5 \quad \text{and} \quad x - y = 10
$$

is represented as:

```python
[[2, 3, 5],
 [1, -1, 10]]
```

Here:

* First row â†’ `[2, 3, 5]` means $2x + 3y = 5$.
* Second row â†’ `[1, -1, 10]` means $1x - 1y = 10$.

The goal is to **eliminate one variable** (say, the first variable) and return the reduced set of equations.

---

### Exercise

Write a function `eliminate_variable(equations, var_index)` that eliminates the variable at position `var_index` (0-based index) from the system of equations. The function should return the reduced system of equations.

* `equations`: List of lists, each inner list represents an equation.
* `var_index`: Integer, the index of the variable to eliminate.
* Return: A new list of equations with one fewer variable.

---

### Example

```python
# Example 1
equations = [[2, 3, 5], [1, -1, 10]]
print(eliminate_variable(equations, 0))
# Output: [[0, 1, -15]]
# Explanation: After eliminating x, we get 0x + 1y = -15 â†’ y = -15

# Example 2
equations = [[1, 1, 4], [2, -1, 1]]
print(eliminate_variable(equations, 1))
# Output: [[1, 0, 5]]
# Explanation: After eliminating y, we get 1x + 0y = 5 â†’ x = 5
```

---
## *Solve Linear Equations Recursively*

**Explanation:**
Equations with multiple unknowns (variables) can be solved step by step. One common way is to **eliminate a variable** from some equations until we are left with a smaller set of equations. Eventually, we reduce it to a single equation with one variable, solve it, and then substitute back to find the remaining variables.

For example, suppose we have two equations:

1. `2x + y = 5`
2. `x - y = 1`

We can **eliminate** one variable, say `y`, to solve for `x`. Then we use the value of `x` to find `y`.

We will use two helper functions:

* `eliminate_variable(equations, var_index)`: removes one variable from the system of equations.
* `solve_for_first_variable(equations)`: directly solves when there is only one variable left.

Using **recursion**, we keep reducing the problem until only one variable remains.

---

**Exercise:**
Write a function `solve_equations(equations)` that:

* Takes `equations` as a list of lists, where each sublist represents one linear equation in the form `[a1, a2, ..., an, b]`, meaning:

  $$
  a_1x_1 + a_2x_2 + \dots + a_nx_n = b
  $$
* Uses `eliminate_variable()` and `solve_for_first_variable()` to recursively solve for all variables.
* Returns the list of variable values.

---

**Example:**

```python
# Example system:
# 2x + y = 5
# x - y = 1
equations = [
    [2, 1, 5],  # 2x + y = 5
    [1, -1, 1]  # x - y = 1
]

solve_equations(equations)  
# Expected Output: [2.0, 1.0]
# Meaning: x = 2, y = 1
```

```python
# Another example:
# x + y + z = 6
# 2y + 5z = -4
# 2x + 5y - z = 27
equations = [
    [1, 1, 1, 6],    # x + y + z = 6
    [0, 2, 5, -4],   # 2y + 5z = -4
    [2, 5, -1, 27]   # 2x + 5y - z = 27
]

solve_equations(equations)  
# Expected Output: [5.0, 3.0, -2.0]
# Meaning: x = 5, y = 3, z = -2
```

---

## Fair Coin Toss

### Explanation:

A coin has two sides: **Head** and **Tail**. If you flip a fair coin, both sides should have an equal chance of appearing. In programming, we can simulate this randomness using the `random` module in Python.

* The function `random.random()` generates a random number between `0.0` (inclusive) and `1.0` (exclusive).
* If the number is less than `0.5`, we can decide it means **Head**.
* If the number is greater than or equal to `0.5`, we can decide it means **Tail**.

In this exercise, you will design a function that:

1. Simulates a fair coin toss.
2. Prints `"HEAD"` if it lands on head, `"TAIL"` if it lands on tail.
3. Returns `0` for head and `1` for tail.

---

### Exercise:

Write a function `fair_coin_toss()` that uses `random.random()` to simulate a fair coin toss.

* If the result is head, print `"HEAD"` and return `0`.
* If the result is tail, print `"TAIL"` and return `1`.

---

### Example:

```python
# Example 1: Suppose random.random() gave 0.23 (< 0.5)
fair_coin_toss()  
# Output:
# HEAD
# Returns: 0

# Example 2: Suppose random.random() gave 0.76 (>= 0.5)
fair_coin_toss()  
# Output:
# TAIL
# Returns: 1
```
---

## Biased Coin Toss

### Explanation:

In a fair coin toss, the chance of getting **Head** or **Tail** is equal â€” each has a probability of 50%.
But sometimes, we want to design a **biased coin** where the chances are not equal.

For example, letâ€™s say we want the coin to show **Head 70% of the time** and **Tail 30% of the time**.
To do this, we can use Pythonâ€™s `random.random()` function, which generates a random number between **0 and 1**.

* If the number is less than `0.7` â†’ we decide itâ€™s a **Head**.
* Otherwise â†’ itâ€™s a **Tail**.

We will also **print** the result (`"Head"` or `"Tail"`) and return `0` for Head and `1` for Tail.

---

### Exercise:

Write a function `biased_coin_toss()` that simulates a coin toss with **70% chance of Head** and **30% chance of Tail**.

* Use `random.random()`.
* Print `"Head"` if itâ€™s Head and `"Tail"` if itâ€™s Tail.
* Return `0` for Head and `1` for Tail.

---

### Example:

```python
# Example usage (your output may vary because of randomness)

biased_coin_toss()
# Output: Head
# Return: 0

biased_coin_toss()
# Output: Tail
# Return: 1
```
---

## Biased Coin Toss with Probability Argument

### Explanation:

A coin toss doesnâ€™t always need to be fair. We can **bias** a coin by changing the probability of getting **Head**.

* If the probability of Head is `p`, then the probability of Tail is `1 - p`.
* We use Pythonâ€™s `random.random()` function, which generates a random number between **0 and 1**.

How it works:

* If the random number is less than `p` â†’ itâ€™s a **Head**.
* Otherwise â†’ itâ€™s a **Tail**.

We will **print** the result (`"Head"` or `"Tail"`) and return:

* `0` for Head
* `1` for Tail

---

### Exercise:

Write a function `biased_coin_toss(p)` that simulates a coin toss with probability `p` for Head.

* Use `random.random()`.
* Print `"Head"` if itâ€™s Head and `"Tail"` if itâ€™s Tail.
* Return `0` for Head and `1` for Tail.

---

### Example:

```python
# Example usage (your output may vary because of randomness)

biased_coin_toss(0.7)
# Output: Head
# Return: 0

biased_coin_toss(0.3)
# Output: Tail
# Return: 1
```
---

## Weighted Choices

### Explanation:

Sometimes, we need to select one item out of many, but not all items have the same chance.
For example:

* Suppose you have three fruits: `["Apple", "Banana", "Cherry"]`
* You want the chance of picking them to be `[0.5, 0.3, 0.2]`

  * Apple â†’ 50% chance
  * Banana â†’ 30% chance
  * Cherry â†’ 20% chance

---

### Exercise:

Write a function `weighted_choice(probabilities)` that:

* Takes a list of probabilities (which should sum to 1).
* Uses `random.random()` to select one index based on the given weights.
* Returns the index of the chosen element.

---

### Example:

```python
# Example usage (your output may vary because of randomness)

weighted_choice([0.5, 0.3, 0.2])
# Possible Return: 0   (with ~50% chance)
# Possible Return: 1   (with ~30% chance)
# Possible Return: 2   (with ~20% chance)

weighted_choice([0.1, 0.1, 0.8])
# Likely Return: 2   (since 80% chance)
```

Hint:
  To implement this, we can use **cumulative probability ranges**:

  * Generate a random number `r` between 0 and 1 using `random.random()`.
  * See which interval `r` falls into.

  Example with `[0.5, 0.3, 0.2]`:

  * If `r < 0.5` â†’ pick index `0` (Apple).
  * If `0.5 <= r < 0.8` â†’ pick index `1` (Banana).
  * If `0.8 <= r < 1.0` â†’ pick index `2` (Cherry).

  The function should return the **index** of the chosen element.

---

## Normalize a List into Probabilities

### Explanation:

Sometimes we have a list of numbers that represent **weights** or **importance**, but they donâ€™t add up to 1.
To convert them into **probabilities**, we need to make sure that:

* Each number is divided by the **total sum** of all numbers.
* The resulting list always adds up to **1.0**.

Example:

* Input: `[1, 2, 2]`
* Sum = `1 + 2 + 2 = 5`
* Divide each element by 5 â†’ `[1/5, 2/5, 2/5]` = `[0.2, 0.4, 0.4]`

This is often used in **machine learning**, **probability models**, and **weighted random choices**.

---

### Exercise:

Write a function `normalize_to_probabilities(numbers)` that:

* Takes a list of numbers.
* Converts them into probabilities so that their sum is `1`.
* Returns the new list of probabilities.

---

### Example:

```python
normalize_to_probabilities([1, 2, 2])
# Output: [0.2, 0.4, 0.4]

normalize_to_probabilities([3, 3, 4])
# Output: [0.3, 0.3, 0.4]
```
---
## Softmax Function

### Explanation:

The **softmax function** is widely used in **machine learning**, especially in classification tasks.
It converts a list of numbers into probabilities, just like normalization, but with an extra twist:

* First, take the **exponential** of each number.
* Then, divide each exponential by the **sum of all exponentials**.

This makes sure:

* All outputs are positive.
* They add up to **1** (so they can be treated as probabilities).

Example:
For numbers `[1, 2, 3]`:

1. Exponentials:

   * `exp(1) â‰ˆ 2.718`
   * `exp(2) â‰ˆ 7.389`
   * `exp(3) â‰ˆ 20.085`

2. Sum = `2.718 + 7.389 + 20.085 = 30.192`

3. Probabilities:

   * `2.718 / 30.192 â‰ˆ 0.09`
   * `7.389 / 30.192 â‰ˆ 0.24`
   * `20.085 / 30.192 â‰ˆ 0.67`

So `[1, 2, 3]` â†’ `[0.09, 0.24, 0.67]`.

---

### Exercise:

Write a function `softmax(values)` that:

* Takes a list of numbers.
* Applies the softmax transformation.
* Returns the resulting list of probabilities.

ðŸ‘‰ Hint: Use `math.exp(x)` for exponentials.

---

### Example:

```python
softmax([1, 2, 3])
# Output: [0.09, 0.24, 0.67]  (approx)

softmax([2, 2, 2])
# Output: [0.33, 0.33, 0.33]
```
---

## Softmax Function (with Numerical Stability)

### Explanation:

The **softmax function** converts a list of numbers into probabilities.
Itâ€™s used in **machine learning** to represent the probability distribution of classes.

**Formula:**

$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

But thereâ€™s a **numerical stability problem**:

* If the numbers are very large (like `1000`), `exp(1000)` will **overflow**.

ðŸ‘‰ Trick: Subtract the maximum value before exponentiation.
This doesnâ€™t change the result because softmax only cares about **relative differences**.

So instead of computing `exp(x)`, we compute:

$$
e^{x_i - \max(x)}
$$

This keeps the numbers smaller and avoids overflow.

---

### Exercise:

Write a function `softmax(values)` that:

1. Finds the maximum number in the list.
2. Subtracts this max from every element.
3. Applies the exponential function.
4. Divides each exponential by the sum of all exponentials.
5. Returns the probabilities as a list.

---

### Example:

```python
softmax([1, 2, 3])
# Output: [0.09, 0.24, 0.67]  (approx)

softmax([1000, 1001, 1002])
# Output: [0.09, 0.24, 0.67]  (approx, stable!)
```

---

## Temperature Scaling

### Explanation:

In machine learning, we sometimes want to **control how confident or uncertain** our probability distribution is.
Thatâ€™s where the **temperature parameter (T)** comes in.

We modify the softmax formula:

$$
\text{softmax}(x_i, T) = \frac{e^{x_i / T}}{\sum_j e^{x_j / T}}
$$

* If **T = 1** â†’ normal softmax.
* If **T < 1** â†’ the distribution becomes **sharper** (higher confidence, one class dominates).
* If **T > 1** â†’ the distribution becomes **flatter** (more uncertainty, probabilities spread out).

Example with values `[1, 2, 3]`:

* `T = 1` â†’ `[0.09, 0.24, 0.67]`
* `T = 0.5` â†’ `[0.02, 0.12, 0.86]` (sharper)
* `T = 2` â†’ `[0.21, 0.31, 0.48]` (flatter)

We should also use the **numerical stability trick** (subtract the max).

---

### Exercise:

Write a function `softmax_with_temperature(values, T)` that:

1. Divides each number by `T`.
2. Subtracts the max value for stability.
3. Applies the exponential function.
4. Normalizes by dividing each exponential by the sum.
5. Returns the probability distribution.

---

### Example:

```python
softmax_with_temperature([1, 2, 3], 1)
# Output: [0.09, 0.24, 0.67]

softmax_with_temperature([1, 2, 3], 0.5)
# Output: [0.02, 0.12, 0.86]  (sharper)

softmax_with_temperature([1, 2, 3], 2)
# Output: [0.21, 0.31, 0.48]  (flatter)
```
---
## Weighted Choices with Temperature

### Explanation

You often need to pick one item from a list where each item has a **weight** (importance). Higher weight â‡’ higher chance to be chosen.
**Temperature (T)** lets you control how **sharp** or **flat** those chances are:

* **T < 1** â†’ **Sharper** distribution (the biggest weights dominate more).
* **T = 1** â†’ Original weighting.
* **T > 1** â†’ **Flatter** distribution (even small weights get more chance).

A simple way to add temperature to positive weights is:

1. Transform each weight as $w_i^{\,1/T}$.
2. Normalize the transformed weights so they sum to 1 (convert to probabilities).
3. Sample an index using these probabilities.

This behaves like softmax with temperature on logits, but works directly on positive weights.

---

### Exercise

Write a function `weighted_choice_with_temperature(weights, T)` that:

* **Arguments**

  * `weights`: a list of non-negative numbers (at least one must be > 0).
  * `T`: a positive float (temperature).

* **Behavior**

  1. Apply temperature scaling: `scaled = [w ** (1.0 / T) for w in weights]`.
  2. Convert `scaled` to probabilities by dividing each by their sum.
  3. Draw a single index according to these probabilities using a single random number (`random.random()`), by walking cumulative sums.
  4. **Return** the selected index (an `int`).

* **Notes / Constraints**

  * If `T <= 0`, you may raise a `ValueError`.
  * If all weights are zero, you may raise a `ValueError`.
  * Use only the standard library (`random` and basic Python).

---

### Example

```python
# Example usage (outputs are stochastic; comments show likely behavior)

# Heavily favors index 2 at lower T, but evens out at higher T
weighted_choice_with_temperature([1, 2, 8], T=0.5)  # Likely returns 2 most of the time
weighted_choice_with_temperature([1, 2, 8], T=1.0)  # Still favors 2, but less sharply
weighted_choice_with_temperature([1, 2, 8], T=2.0)  # Flatter; 0 and 1 get picked more often

# Equal weights stay equal at any T
weighted_choice_with_temperature([3, 3, 3], T=0.3)  # ~uniform over {0,1,2}
weighted_choice_with_temperature([3, 3, 3], T=2.0)  # ~uniform over {0,1,2}
```

---

## Flatten an Array

### Explanation:

Sometimes, lists (or arrays) in Python are not just simple lists of numbers but contain other lists inside them. Such lists are called **nested lists**.
For example: `[1, [2, 3], [4, [5, 6]]]`

To make it easier to work with, we might want to **flatten** the listâ€”meaning we turn it into a single list with no nesting:
`[1, 2, 3, 4, 5, 6]`.

A powerful way to solve this problem is by using **recursion**.
Recursion means a function calls itself to solve a smaller version of the problem. In this case:

* If the element is a number, just add it to the result.
* If the element is a list, flatten that list (by calling the function again) and add its elements.

---

### Exercise:

Write a function `flatten_list(nested_list)` that takes a possibly nested list of numbers and returns a flat list (all elements in a single list).

ðŸ’¡ **Hint:** Use recursion. When you see another list inside, call `flatten_list` again.

---

### Example:

```python
flatten_list([1, [1, 2, [3, 4]]])  
# Output: [1, 1, 2, 3, 4]

flatten_list([1, [2, [3, [4, 5]]]])  
# Output: [1, 2, 3, 4, 5]
```
---

## Solve Expression (Array Form)

### Explanation:

In some problems, instead of writing math expressions like `"(20 + 40) * 90"`, the expression is stored in a special way: as **arrays (lists) of three elements**.

* The **first element** is an operator (`+`, `-`, `*`, `/`).
* The **second and third elements** are the operands.
* An operand can be:

  * a **number**, like `20`, or
  * another **nested array**, which is itself an expression.

For example:

`["*", ["+", 20, 40], 90]`

This means:

1. First solve `["+", 20, 40]` â†’ 60
2. Then solve `["*", 60, 90]` â†’ 5400

So, the result is **5400**.

To solve this, you can use **recursion**:

* If the input is just a number, return it.
* Otherwise, solve the left operand, solve the right operand, then apply the operator.

---

### Exercise:

Write a function `solve_expression(expr)` that takes an expression (in this special array format) and returns its value.

ðŸ’¡ **Hint:** Use recursion to handle nested expressions.

---

### Example:

```python
solve_expression(["*", ["+", 20, 40], 90])  
# Output: 5400

solve_expression(["-", ["*", ["/" 100 10], 5], 15])  
# Output: 35   (because ((100/10)*5) - 15 = 50 - 15)
```
---
