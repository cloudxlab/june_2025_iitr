## *Compute Mean of a List of Numbers*

**Explanation:**
The *mean* (often called the *average*) of a list of numbers is found by adding up all the numbers and then dividing by how many numbers there are.
For example, if we have the numbers `[2, 4, 6, 8]`, the sum is `2 + 4 + 6 + 8 = 20`.
There are `4` numbers in the list.
So, the mean is `20 / 4 = 5`.

This is a useful way to find the central value of a group of numbers.

---

**Exercise:**
Write a function `compute_mean(numbers)` that takes a list of numbers and returns their mean (average).
If the list is empty, return `0`.

---

**Example:**

```python
compute_mean([2, 4, 6, 8])      # Output: 5.0
compute_mean([10, 20, 30])      # Output: 20.0
compute_mean([])                # Output: 0
```
---
## *Compute Standard Deviation (SD) of a List of Numbers*

**Explanation:**
The *standard deviation* (SD) is a way to measure how spread out numbers are in a list.

* If the numbers are very close to each other, the SD will be small.
* If the numbers are spread far apart, the SD will be large.

To compute the SD:

1. Find the *mean* (average) of the numbers.
2. Subtract the mean from each number and square the result.
3. Find the average of these squared differences.
4. Take the square root of that value.

For example, for the list `[2, 4, 4, 4, 5, 5, 7, 9]`:

* Mean = 5
* Squared differences = \[9, 1, 1, 1, 0, 0, 4, 16]
* Average of squared differences = 4
* Standard Deviation = âˆš4 = 2

---

**Exercise:**
Write a function `compute_sd(numbers)` that takes a list of numbers and returns the standard deviation.

---

**Example:**

```python
compute_sd([2, 4, 4, 4, 5, 5, 7, 9])   # Output: 2.0
compute_sd([10, 10, 10, 10])           # Output: 0.0
```
---

## *Compute Root Mean Square Error (RMSE) in n-dimensions*

**Explanation:**
When we try to measure how close our predicted values are to the actual values, we use something called the *Root Mean Square Error (RMSE)*. It tells us, on average, how far our predictions are from the actual values.

The RMSE is calculated in three steps:

1. Subtract each predicted value from the actual value to find the **error**.
2. Square each error (to make them positive).
3. Find the **average** of these squared errors.
4. Take the **square root** of this average.

In *n-dimensions*, both the actual and predicted values are given as lists (or vectors). For example, if the actual values are `[2, 3, 4]` and the predicted values are `[3, 2, 5]`, the errors are `[2-3, 3-2, 4-5] = [-1, 1, -1]`. Squaring them gives `[1, 1, 1]`. The average is `1`, and the square root of `1` is `1`. So, the RMSE is `1`.

---

**Exercise:**
Write a function `compute_rmse(actual, predicted)` that:

* Takes two lists of equal length: `actual` and `predicted`.
* Returns the Root Mean Square Error between them.

---

**Example Usage:**

```python
compute_rmse([2, 3, 4], [3, 2, 5])  
# Output: 1.0  

compute_rmse([1, 2, 3], [1, 2, 3])  
# Output: 0.0  
```
---
## *Compute Mean Absolute Error in N-Dimensions*

**Explanation:**
The **Mean Absolute Error (MAE)** is a way of measuring how far our predictions are from the actual values.
It is calculated by taking the **average of the absolute differences** between predicted values and actual values.

For example, if the true values are `[3, 5, 2]` and the predicted values are `[2, 5, 4]`, the absolute errors are:

* |3 - 2| = 1
* |5 - 5| = 0
* |2 - 4| = 2

So the MAE is:

$$
\text{MAE} = \frac{1 + 0 + 2}{3} = 1
$$

In **N-dimensions**, each point can have multiple coordinates. The error is computed for each coordinate, then averaged across all points.

---

**Exercise:**
Write a function `compute_mae(actual, predicted)` that:

* Takes two lists of points (each point is a list of numbers, e.g. `[x, y, z, ...]`).
* Returns the mean absolute error across all dimensions.

---

**Example:**

```python
# Example 1: 1-D
actual = [3, 5, 2]
predicted = [2, 5, 4]
compute_mae(actual, predicted)  
# Output: 1.0

# Example 2: 2-D
actual = [[1, 2], [3, 4], [5, 6]]
predicted = [[2, 2], [2, 5], [5, 7]]
compute_mae(actual, predicted)  
# Output: 0.8888888888888888
```

ðŸ‘‰ In the second example, the function computes absolute differences for each coordinate, sums them, and divides by total number of values (not just points).

---

## *Compute Huber Loss for a Dataset*

**Explanation:**
When we build a machine learning model, we test it on many data points. To see how well the model is doing, we calculate the **loss** (difference between prediction and actual value).

The **Huber Loss** is a special function that:

* Uses **squared error** when the prediction is close to the actual value (small error).
* Uses **absolute error** when the prediction is far away (big error).

This makes it less sensitive to outliers than plain squared error.

The formula for one prediction is:

$$
L(y, f) = 
\begin{cases} 
0.5 \cdot (y - f)^2 & \text{if } |y - f| \leq \delta \\ 
\delta \cdot |y - f| - 0.5 \cdot \delta^2 & \text{if } |y - f| > \delta 
\end{cases}
$$

To find the loss for a dataset:

1. Compute this formula for each pair of actual and predicted values.
2. Add them all up.
3. Divide by the number of data points to get the **average loss**.

---

**Exercise:**
Write a function `compute_huber_loss(y_true, y_pred, delta)` that takes:

* `y_true`: a list of actual values
* `y_pred`: a list of predicted values
* `delta`: the threshold value

and returns the **average Huber loss**.

---

**Hints for Beginners:**

1. Start with an empty list to store the loss for each data point.
2. Loop over both `y_true` and `y_pred` together (hint: use `zip`).
3. For each pair `(y, f)` do the following:

   * Calculate the error: `error = y - f`.
   * If `abs(error) <= delta`, compute `0.5 * error**2`.
   * Otherwise, compute `delta * abs(error) - 0.5 * delta**2`.
   * Append this value to your list.
4. At the end, find the average by dividing the sum of all losses by the length of the list.

---

**Example Usage:**

```python
compute_huber_loss([5, 2, 7], [4.8, 2.5, 10], 1)
# For (5, 4.8): 0.5*(0.2^2) = 0.02
# For (2, 2.5): 0.5*(0.5^2) = 0.125
# For (7, 10): 1*3 - 0.5*1^2 = 2.5
# Average = (0.02 + 0.125 + 2.5) / 3 = 0.8817
# Output: 0.8817 (approximately)

compute_huber_loss([1, 2, 3], [1, 2, 3], 1)
# All predictions are exact â†’ loss = 0
# Output: 0
```

---
## *Check if a Point is Closer to A or B in N-Dimension*

**Explanation:**
In mathematics, a *point* in an N-dimensional space can be represented as a list (or tuple) of numbers. For example:

* A point in 2D is like `(x, y)` â†’ e.g., `(3, 4)`
* A point in 3D is like `(x, y, z)` â†’ e.g., `(2, 1, 5)`
* In N-dimensions, itâ€™s just a list with N numbers.

To find how close two points are, we calculate the **distance** between them. The most common way is using the **Euclidean distance** formula:

$$
\text{distance}(P, A) = \sqrt{(p_1 - a_1)^2 + (p_2 - a_2)^2 + ... + (p_n - a_n)^2}
$$

We can compare the distance from point `P` to `A` and from `P` to `B`.

* If `distance(P, A) < distance(P, B)`, then P is closer to A.
* Otherwise, P is closer to B (or equally distant if they are the same).

---

**Exercise:**
Write a function `closer_point(P, A, B)` that:

* Takes three points (`P`, `A`, and `B`) as lists of numbers (same length).
* Returns `"A"` if P is closer to A, `"B"` if P is closer to B, or `"Equal"` if the distances are the same.

---

**Example:**

```python
closer_point([1, 2], [0, 0], [5, 5])  
# Output: "A"  (because P is closer to A)

closer_point([3, 3, 3], [0, 0, 0], [6, 6, 6])  
# Output: "Equal"  (distances are the same)

closer_point([10, 10], [2, 2], [20, 20])  
# Output: "B"  (P is closer to B)
```

Would you like me to also include a **step-by-step solution code** (with comments for learners), or keep it just as an exercise statement?

----
## *Find Nearest Neighbour in 1D*

**Explanation:**
Imagine you have a list of numbers on a line (like houses along a street). If you are standing at a certain position (a target number), you may want to know which house (number) is closest to you. This closest number is called the *nearest neighbour*.

For example, if the numbers are `[2, 5, 8, 12]` and the target is `6`, the nearest neighbour is `5` because the distance from `6` to `5` is `1`, while the distance from `6` to `8` is `2`.

We measure distance as the absolute difference:
`distance = |target - number|`

---

**Exercise:**
Write a function `find_nearest_neighbour(numbers, target)` that returns the nearest neighbour of the target from the list.

---

**Example:**

```python
find_nearest_neighbour([2, 5, 8, 12], 6)   # Output: 5
find_nearest_neighbour([1, 4, 10, 20], 15) # Output: 10
```

---
## *Find Nearest Neighbour in 2D*

**Explanation:**
Imagine you are standing on a map at a certain location, and there are several other points (like stores or landmarks) around you. To figure out which one is closest, you measure the *distance* from your location to each of the other points. The point with the smallest distance is called the **nearest neighbour**.

In 2D, each point has two coordinates:

* `x` (horizontal position)
* `y` (vertical position)

The **distance** between two points `(x1, y1)` and `(x2, y2)` is calculated using the **Euclidean distance formula**:

$$
\text{distance} = \sqrt{(x2 - x1)^2 + (y2 - y1)^2}
$$

For example, the distance between `(0, 0)` and `(3, 4)` is:

$$
\sqrt{(3-0)^2 + (4-0)^2} = \sqrt{9 + 16} = 5
$$

---

**Exercise:**
Write a function `find_nearest_neighbour(points, target)` that returns the point from the list `points` which is closest to the `target`.

* `points` is a list of `(x, y)` tuples.
* `target` is a tuple `(x, y)` representing the location we want to compare.
* The function should return the nearest neighbour point as a tuple.

---

**Example Usage:**

```python
find_nearest_neighbour([(1, 2), (3, 4), (6, 1)], (2, 3))
# Output: (1, 2)

find_nearest_neighbour([(0, 0), (5, 5), (2, 1)], (3, 3))
# Output: (2, 1)
```

---

## *Find Nearest Neighbour in N-Dimensions*

**Explanation:**
Imagine you are standing in a city and want to know which shop is closest to you. To find this, you calculate the *distance* between your location and each shopâ€™s location, and then pick the one with the smallest distance.

In computer science, we represent these locations as *points* with coordinates.

* In **2D**, a point might look like `[x, y]` (like a map).
* In **3D**, it could be `[x, y, z]` (like in space).
* In general, in **N dimensions**, a point is `[a1, a2, ..., an]`.

The **Euclidean distance** formula helps us measure how far apart two points are:

$$
\text{distance}(A, B) = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_n)^2}
$$

The nearest neighbour of a point is simply the one with the *smallest distance*.

---

**Exercise:**
Write a function `find_nearest_neighbour(point, points)` that takes:

* `point`: a list of numbers representing coordinates of a point in N-dimensions
* `points`: a list of points (each a list of numbers)

and returns the point from `points` that is nearest to `point`.

---

### Hints (Step-by-Step)

1. **How to calculate distance between two points?**

   * Subtract each coordinate of one point from the corresponding coordinate of the other.
   * Square these differences.
   * Add them all together.
   * Take the square root of the result.

   Example: distance between `[1,2]` and `[3,4]` is
   $\sqrt{(1-3)^2 + (2-4)^2} = \sqrt{4+4} = \sqrt{8}$.

---

2. **How to apply this to a list of points?**

   * For each point in the list, compute its distance to the given point.
   * Keep track of the *smallest distance* found so far.
   * Also remember the corresponding point.

---

3. **How to find the nearest neighbour?**

   * After checking all the points, the one with the smallest distance is the nearest neighbour.

---

**Example Usage:**

```python
find_nearest_neighbour([1, 2], [[3, 4], [2, 1], [0, 0]])
# Output: [2, 1]

find_nearest_neighbour([0, 0, 0], [[1, 1, 1], [2, 2, 2], [-1, -1, -1]])
# Output: [1, 1, 1]
```






---
## *Solve for One Variable in a Linear Equation*

**Explanation:**
A linear equation with multiple variables looks like this:

$$
a_1x_1 + a_2x_2 + a_3x_3 + \dots + a_nx_n = b
$$

Here, each $a_i$ is a coefficient, $x_i$ is a variable, and $b$ is the right-hand side value.
If you already know the values of all variables except one, you can find the unknown variable by rearranging the equation.

For example, consider the equation:

$$
3x + 4y + 6z = 20
$$

If you know $y = 5$ and $z = 6$, then substitute these values:

$$
3x + 4(5) + 6(6) = 20
$$

$$
3x + 20 + 36 = 20
$$

$$
3x = 20 - 56 = -36
$$

$$
x = -12
$$

We represent this using two lists:

* `equation = [3, 4, 6, 20]` â†’ coefficients and RHS value.
* `vars = [5, 6]` â†’ known variable values (in the same order as coefficients after the first one).

So the formula is:

$$
x = \frac{b - (a_2 \cdot var_1 + a_3 \cdot var_2 + \dots)}{a_1}
$$

---

**Exercise:**
Write a function `solve_for_first_variable(equation, vars)` that returns the value of the first variable.

* `equation`: a list of numbers where the first $n$ elements are coefficients of the variables and the last element is the right-hand side value.
* `vars`: a list of values for the last $n-1$ variables.

---

**Example:**

```python
solve_for_first_variable([3, 4, 6, 20], [5, 6])  
# Output: -12  

solve_for_first_variable([2, 5, 7], [4])  
# Equation: 2x + 5y = 7, y=4  
# (7 - (5*4)) / 2 = (7 - 20) / 2 = -13/2 = -6.5  
# Output: -6.5
```

---
## *Eliminate a Variable from Equations*

**Explanation:**
In algebra, one way to solve systems of equations is called *elimination*.
The idea is to combine two equations in such a way that one variable â€œdisappears.â€ This leaves us with an equation that has one fewer variable, which is easier to solve.

For example, consider these two equations:

1. `2x + 3y = 8`
2. `4x - y = 2`

We want to eliminate `x`.

**Step 1:** Look at the coefficients of `x`. They are 2 (from the first equation) and 4 (from the second).
**Step 2:** Multiply the first equation by 2, so the coefficient of `x` becomes 4.

* Equation 1 becomes: `4x + 6y = 16`
  **Step 3:** Subtract Equation 2 from the new Equation 1:
* `(4x + 6y) - (4x - y) = 16 - 2`
* `7y = 14`

Now, the new equation has only one variable (`y`).

---

**Exercise:**
Write a function `eliminate_variable(eq1, eq2, var_index)` that eliminates the variable at position `var_index` from the two given equations.

* Each equation is represented as a list of numbers: the coefficients of variables followed by the constant term.
* Example: The equation `2x + 3y = 8` is represented as `[2, 3, 8]`.
* The function should return a new equation (list) with one fewer variable.

**Hints for Learners:**

1. Find the coefficients of the variable you want to eliminate in both equations.
2. Multiply each equation so that these coefficients become equal.
3. Subtract one equation from the other to cancel out the chosen variable.
4. The result is a new equation with one fewer variable.

---

**Example Usage:**

```python
eliminate_variable([2, 3, 8], [4, -1, 2], 0)
# Output: [7, 14]   # Represents 7y = 14

eliminate_variable([1, 2, 3], [3, 1, 7], 1)
# Output: [-8, -14]   # Represents -8x = -14
```

---
**Topic:** *Solve Linear Equations Recursively*

**Explanation:**
Equations with multiple unknowns (variables) can be solved step by step. One common way is to **eliminate a variable** from some equations until we are left with a smaller set of equations. Eventually, we reduce it to a single equation with one variable, solve it, and then substitute back to find the remaining variables.

For example, suppose we have two equations:

1. `2x + y = 5`
2. `x - y = 1`

We can **eliminate** one variable, say `y`, to solve for `x`. Then we use the value of `x` to find `y`.

We will use two helper functions:

* `eliminate_variable(equations, var_index)`: removes one variable from the system of equations.
* `solve_for_first_variable(equations)`: directly solves when there is only one variable left.

Using **recursion**, we keep reducing the problem until only one variable remains.

---

**Exercise:**
Write a function `solve_equations(equations)` that:

* Takes `equations` as a list of lists, where each sublist represents one linear equation in the form `[a1, a2, ..., an, b]`, meaning:

  $$
  a_1x_1 + a_2x_2 + \dots + a_nx_n = b
  $$
* Uses `eliminate_variable()` and `solve_for_first_variable()` to recursively solve for all variables.
* Returns the list of variable values.

---

**Example:**

```python
# Example system:
# 2x + y = 5
# x - y = 1
equations = [
    [2, 1, 5],  # 2x + y = 5
    [1, -1, 1]  # x - y = 1
]

solve_equations(equations)  
# Expected Output: [2.0, 1.0]
# Meaning: x = 2, y = 1
```

```python
# Another example:
# x + y + z = 6
# 2y + 5z = -4
# 2x + 5y - z = 27
equations = [
    [1, 1, 1, 6],    # x + y + z = 6
    [0, 2, 5, -4],   # 2y + 5z = -4
    [2, 5, -1, 27]   # 2x + 5y - z = 27
]

solve_equations(equations)  
# Expected Output: [5.0, 3.0, -2.0]
# Meaning: x = 5, y = 3, z = -2
```

---